{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StreetView_ImageClassification\n",
    "\n",
    "This program classifies images of house facades using a Convolutional Neural Network (CNN)\n",
    "Output has two classes: \n",
    "    SV (Pictures taken from Street View screenshots) \n",
    "    REAL (Real pictures taken in front of the house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### IMPORTANT ######\n",
    "# Enter the directory in which you cloned this repository\n",
    "path2repo=\"/opt/sasinside/DemoData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and connection to the CAS server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from swat import *\n",
    "from PIL import Image, ImageStat\n",
    "\n",
    "path=path2repo+'StreetView_ImageClassification'\n",
    "cashost='localhost'\n",
    "casport=5570\n",
    "casauth='~/.authinfo'\n",
    "s = CAS(cashost, casport, caslib=\"cas\")\n",
    "s.addcaslib(path=path,\n",
    "            caslib='sv_cnn_demo',\n",
    "            description='Street View classification CNN',\n",
    "            subdirs=True, #for acces to image directories\n",
    "            session=False,  #global CASLIB \n",
    "            activeonadd=True)\n",
    "\n",
    "s.setsessopt(caslib='sv_cnn_demo')\n",
    "\n",
    "s.loadactionset(actionset=\"image\")\n",
    "s.loadactionset(actionset=\"table\")\n",
    "s.loadactionset(actionset=\"astore\")\n",
    "s.loadactionset(actionset=\"deepLearn\")\n",
    "s.loadactionset(actionset=\"sampling\")\n",
    "s.loadactionset(actionset='freqtab')\n",
    "\n",
    "seed=123\n",
    "\n",
    "set_option(print_messages=True)\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, partition, pre-process and augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "\n",
    "s.loadImages(casOut={'name':'Images','replace':True},\n",
    "             path=path+'/IMAGES',\n",
    "             quiet=True,\n",
    "             recurse=True,   # This line and the next recurse over subdirectories \n",
    "             labelLevels=-1) # and assign labels accordingly (REAL or SV)\n",
    "\n",
    "s.stratified(table={\"name\":\"Images\",\"groupBy\":{\"_label_\"}},\n",
    "             output={\"casOut\":{\"name\":\"Images\",\"replace\":True},\"copyVars\":\"ALL\"},\n",
    "             samppct=25,   #Validation partition percentage\n",
    "             partind=True, #Create binary variable _PartInd_. 0=Training set, 1=Validation set\n",
    "             seed=seed)       \n",
    "\n",
    "s.processImages(casOut={'name':'Images','replace':True},\n",
    "                table='Images',\n",
    "                copyVars='_PartInd_',\n",
    "                imageFunctions=[\n",
    "                        {'options':{'functionType':'CONVERT_COLOR','type':'COLOR2GRAY'}},        #Grayscale\n",
    "                        {'options':{'functionType':'SOBEL','dx':1,'dy':1,'kernelsize':'SIZE3'}}, #Sobel edge detection\n",
    "                        {'options':{'functionType':'RESIZE','width':1120,'height':1120}}])       #Resize to match CNN input layer\n",
    "\n",
    "s.augmentImages(casOut={'name':'Images','replace':True}, #Image augmentation (3X)\n",
    "                table='Images',\n",
    "                copyVars='_PartInd_',\n",
    "                cropList=[{'useWholeImage':True,'mutations':{'horizontalFlip':True}}, #Horizontal mirror\n",
    "                          {'useWholeImage':False,'x':10,'y':10,'w':1100,'h':1100,'outW':1120,'outH':1120}]) #Crop & resample\n",
    "\n",
    "\n",
    "print('\\nPre-processing time:',round((time.time()-t0)/60,2),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Convolutional Neural Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The defined architecture has four groups of layers, each grou consisting of:\n",
    "\n",
    "##Convolutional layer\n",
    "##Batch normalization layer\n",
    "##Pooling layer\n",
    "\n",
    "##The input layer takes images of size 1120x1120x1 (Gray scale)\n",
    "##The output layer has two neurons, each one fully connected to the last pooling layer (pool4)\n",
    "\n",
    "modelName='CNN_SV'\n",
    "\n",
    "# Create a CNN\n",
    "s.buildModel(model=dict(name=modelName,replace=True),type='CNN')\n",
    "\n",
    "# input layer: 1channel, \n",
    "s.addLayer(model=modelName, name='data',\n",
    "          layer=dict( type='input', nchannels=1, width=1120, height=1120)) \n",
    "\n",
    "# pool0 layer: 1 channel, 5x5 pooling, output = 224 x 224 */\n",
    "s.addLayer(model=modelName, name='pool0',\n",
    "          layer=dict(type='pooling',width=5, height=5, stride=5, pool='max'), \n",
    "          srcLayers=['data'])\n",
    "\n",
    "#------------------FIRST LAYER-------------------------------\n",
    "\n",
    "# conv1 layer: 6 channels, 7x7 conv, stride=2; output = 112 x 112 */\n",
    "s.addLayer(model=modelName, name='conv1',\n",
    "          layer=dict( type='convolution', nFilters=10, width=3, height=3, \n",
    "                      stride=2, act='identity'), \n",
    "          srcLayers=['pool0'])\n",
    "\n",
    "# bn_conv1 batch norm layer: 6 channels, output = 112 x 112 */\n",
    "s.addLayer(model=modelName, name='bn_conv1',\n",
    "          layer=dict( type='batchnorm', act='relu'), \n",
    "          srcLayers=['conv1'])\n",
    "\n",
    "# pool1 layer: 6 channels, 3x3 pooling, output = 56 x 56 */\n",
    "s.addLayer(model=modelName, name='pool1',\n",
    "          layer=dict(type='pooling',width=3, height=3, stride=2, pool='max'), \n",
    "          srcLayers=['bn_conv1'])\n",
    "\n",
    "#------------------SECOND LAYER-------------------------------\n",
    "\n",
    "# conv2 layer: 20 channels, 3x3 conv, output = 28 x 28\n",
    "s.addLayer(model=modelName, name='conv2',\n",
    "          layer=dict(type='convolution', nFilters=20, width=3, height=3, \n",
    "                     stride=2, noBias=True, act='identity'), \n",
    "          srcLayers=['pool1'])\n",
    "\n",
    "# bn_conv2 batch norm layer: 20 channels, output = 28 x 28\n",
    "s.addLayer(model=modelName, name='bn_conv2',\n",
    "          layer=dict( type='batchnorm', act='relu'), \n",
    "          srcLayers=['conv2'])\n",
    "\n",
    "#pool2 layer: 20 channels, output = 14 x 14\n",
    "s.addlayer(model=modelName, name='pool2',\n",
    "           layer=dict( type='pooling',width=2,height=2,stride=2,pool='max'),\n",
    "           srcLayers=['bn_conv2'])\n",
    "\n",
    "#------------------THIRD LAYER-------------------------------\n",
    "\n",
    "# conv3 layer: 128 channels, 1x1 conv, output = 14 x 14\n",
    "s.addLayer(model=modelName, name='conv3',\n",
    "          layer=dict( type='convolution', nFilters=128, width=1, height=1, \n",
    "                      stride=1, noBias=True, act='identity'), \n",
    "          srcLayers=['pool2'])\n",
    "\n",
    "# bn_conv3 batch norm layer: 128 channels, output = 14 x 14\n",
    "s.addLayer(model=modelName, name='bn_conv3',\n",
    "          layer=dict( type='batchnorm', act='relu'), \n",
    "          srcLayers=['conv3'])\n",
    "\n",
    "#pool3  outout = 7 x 7\n",
    "s.addlayer(model=modelName, name='pool3',\n",
    "           layer=dict( type='pooling',width=2,height=2,stride=2,pool='mean'),\n",
    "           srcLayers=['bn_conv3'])\n",
    "\n",
    "#------------------FOURTH LAYER-------------------------------\n",
    "\n",
    "# conv4 layer: 256 channels, 1x1 conv, output = 7 x 7\n",
    "s.addLayer(model=modelName, name='conv4',\n",
    "          layer=dict( type='convolution', nFilters=256, width=1, height=1, \n",
    "                      stride=1, noBias=True, act='identity'), \n",
    "          srcLayers=['pool3'])\n",
    "\n",
    "#bn_conv4 batch norm layer: 128 channels, output = 7 x 7\n",
    "s.addLayer(model=modelName, name='bn_conv4',\n",
    "          layer=dict( type='batchnorm', act='relu'), \n",
    "          srcLayers=['conv4'])\n",
    "\n",
    "# pool4 layer: 256 channels, 7x7 pooling, output = 1 x 1\n",
    "s.addLayer(model=modelName, name='pool4',\n",
    "          layer=dict(type='pooling',width=7, height=7, stride=7, pool='mean'), \n",
    "          srcLayers=['bn_conv4'])\n",
    "\n",
    "#------------------OUTPUT LAYER-------------------------------\n",
    "\n",
    "# fc2 output layer: 2 neurons \n",
    "s.addLayer(model=modelName, name='fc2',\n",
    "          layer=dict(type='output',n=2, act='softmax'), \n",
    "          srcLayers=['pool4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you do not wish to train the network, you can use load the pre-trained weights\n",
    "\n",
    "weights='PreTrained1' ## Weight file to use\n",
    "\n",
    "\n",
    "s.loadTable(path='Weights/'+weights+'.csv',\n",
    "            casout={'name':'CNN_SV_WEIGHTS','replace':True})\n",
    "s.loadTable(path='Weights/ATTRS.sashdat',casout={'name':'attrs','replace':True})\n",
    "s.table.attribute(task = \"ADD\", \n",
    "                  name = 'CNN_SV_WEIGHTS', \n",
    "                  attrtable = \"attrs\"\n",
    "                  )\n",
    "\n",
    "\n",
    "s.dlscore(model='CNN_SV',\n",
    "          initWeights={'name':'CNN_SV_WEIGHTS'},\n",
    "          table={'name':\"Images\"},\n",
    "          casout={'name':'ScoredValidation','replace':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights are randomly initialized..\n",
    "\n",
    "t0=time.time()\n",
    "\n",
    "train=s.deepLearn.dlTrain(inputs=[{\"name\":\"_image_\"}],\n",
    "                    modelTable={\"name\":modelName},\n",
    "                    modelWeights={\"name\":\"CNN_SV_WEIGHTS\",\"replace\":True},\n",
    "                    table={\"name\":\"Images\",\"where\":\"_PartInd_=0\"}, #Use training set\n",
    "                    seed=seed,\n",
    "                    optimizer={\"algorithm\":{\"method\":\"ADAM\",\n",
    "                                            \"learningRate\":0.0005,\n",
    "                                            \"learningRatePolicy\":\"POLY\", #Gradualy reduces learning rate.\n",
    "                                            \"power\":.50, #Regulating learning rate decrease\n",
    "                                            \"useLocking\":True}, #Synchronous SGD\n",
    "                               \"maxEpochs\":12,#Iterations over entire dataset\n",
    "                               \"dropout\":.5, #Dropout for reducing chances overfitting\n",
    "                               #\"regL1\":5e-5, #Regularization \n",
    "                               #\"regL2\":.1,\n",
    "                               \"logLevel\":3, #Verbosity (1,2,3)\n",
    "                               \"totalminiBatchSize\":20, #Batch size\n",
    "                               \"seed\":seed #Random seed\n",
    "                              },\n",
    "                    validTable={\"name\":\"Images\",\"where\":\"_PartInd_=1\"}, #Validation set\n",
    "                    target=\"_label_\" #Target variable name\n",
    "                    )\n",
    "print(train)\n",
    "\n",
    "tt=time.time()\n",
    "print('\\nTiempo de entrenamiento:',round((tt-t0)/60,2),'minutos')\n",
    "\n",
    "## Score training set\n",
    "score_t=s.dlscore(model='CNN_SV',\n",
    "          initWeights={'name':'CNN_SV_WEIGHTS'},\n",
    "          table={'name':\"Images\",'where':'_PartInd_=0'},\n",
    "          casout={'name':'ScoredTraining','replace':True})\n",
    "\n",
    "## Score validation set\n",
    "score_v=s.dlscore(model='CNN_SV',\n",
    "          initWeights={'name':'CNN_SV_WEIGHTS'},\n",
    "          table={'name':\"Images\",'where':'_PartInd_=1'},\n",
    "          casout={'name':'ScoredValidation','replace':True})\n",
    "print(score_t['ScoreInfo'])\n",
    "print(score_v['ScoreInfo'])\n",
    "print('\\nTiempo de evaluación:',round((time.time()-tt)/60,2),'minutos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='myWeights' #Modificar nombre del archivo\n",
    "s.save(table=\"CNN_SV_WEIGHTS\",name='Weights/'+name+'.csv',replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar as ASTORE and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.dlExportmodel(casout={'name':'ASTORE','replace':True},\n",
    "                initweights='CNN_SV_WEIGHTS',\n",
    "                modeltable='CNN_SV')\n",
    "\n",
    "s.score(casout={'name':'ASTORE_SCORE','replace':True},\n",
    "        copyVars='_path_',\n",
    "        table={'name':'Images','where':'_partInd_=1'},\n",
    "        rstore='ASTORE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
